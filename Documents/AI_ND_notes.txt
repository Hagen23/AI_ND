AI Nanodegree
----------------

Anaconda
--------

- Using anaconda and python3 for the projects.

- With Anaconda, it's simple to install the packages you'll often use in data science work. 

- Conda is both a package manager and an environment manager. You can set up environments with different package versions (even python versions) if need be, to isolate a project.

- To list the packages that are installed >> conda list
- To show the conda version >> conda --version
- To update conda >> conda update conda
- To update all the packages >> conda upgrade --all
- To install a package, and the requiered dependencies >> conda install <package name / names>
- To uninstall a package >> conda remove <package name>
- To search for a package >> conda search <search term>
- Apparently, conda redownloads packages for each environment -

- Creating environments
	
	- conda create --name <name> <list of packages>
	- conda create -n <name> <list of packages>
	- A specific version of python can also be used >> conda create -n <name> python=<version>

- To list the created environments >> conda env list

- To enter an environment

	- On Linux/MAC OS >> source activate <environment name>
	- On Windows >> activate <environment name>

- To leave an environment 

	- On Linux/MAC OS >> source deactivate
	- On Windows >> deactivate

- To remove an environment >> conda env remove -n <environment name>

- An environment can be saved so that it can be shared, and all the necesary packages, with the correct versions, are installed. 
- The packages are saved to a YAML file with >> conda env export > <filename>.yaml
- To create an environment from a file >> conda env create -f <filename>.yaml

- Best practices

	- Using environments for each project.
	- Add the environment to a github repository.

- Anaconda + opencv

	conda create -n opencv numpy scipy scikit-learn matplotlib python=3
	source activate opencv
	conda install -c https://conda.binstar.org/menpo opencv3

- To return to the default instalation of python, just change the path environment variable.

Constraint propagation and search
---------------------------------

- Solving any sudoku puzzle.

- Constraint propagation

	- When trying to solve a problem, there may be some local constraints to each square. These contraints help narrow the possibilities for the answer. This technique helps to extract the maximum information out of the constraints in order to get close to the solution. Simple constraints can be iteratively applied to narrow the search space of solutions. Constraint propagation can be used to solve problems such as calendar scheduling, and cryptographic puzzles.
	
- Search

	- If there is a point where two or more solutions are possible, we can branch and consider them all. We can continue the branching until we create a whole tree of possibilities and find ways to traverse the tree until a solution is found.

- Labeling for the Sudoku solving agent. 

	- Rows >> A B C D E F G H I
	- Cols >> 1 2 3 4 5 6 7 8 9
	- Individual squares at the intersection of rows and columns >> boxes with labels A1, I9, etc
	- Complete rows, columns, 3x3 squares >> units; each unit is a set of 9 boxes; there are 27 units
	- For a particular box, its peers will be all the other boxes that belong to a common unit

- Strategies

	- Elimination
		If a box has a value assigned, then none of the peers of this box can have this value.
	- Only one
		If only one box of a unit has a possible value, it most likely is the correct option.
	- To apply constraint propagation, it is needed to perform the elimination and only one strategies in a loop until no changes were made to the grid.
	

Heuristics
-----------

- Some additional piece of information - a rule, function, or constraint - that informs an otherwise brute-force algorithm to act in a more optimal manner.

Search 
----------

- Used, for example, in Navigation.

- Pruning the search tree with an heuristic makes it so that certain options of the search tree are not considered since they do not contribute to a specific goal.

- Adversarial search
	- Mini-max algorithm: You are trying to maximize your chances of winning on your turn, and your opponent is trying to minimize your chances of winning on their turn.

- A* search

Intelligent systems
-------------------

- Environment is where agents perform actions.
	- Can be:
		- Fully or partially observable.
		- Deterministic (know for sure about the results of each action) or stochastic (thre is some uncertainty in the actions).
		- Discrete (finite number of states) or continuous (infinit number of states).
		- Benign (only the agent is taking actions that affect its goal) or adversarial (one or more agents that try to defeat its goal)
	
- State: Represent only the elements relevant to solving a problem
	- Goal state
	
- An agent interacts with the environments by sensing its properties using sensores, this is called Perception. An agent produces useful output, or actions, that typically change the state of the environment. The proces with which an agent takes a desicion based on its perception is called cognition.

- An intelligent agent is one that takes actions to maximize its expected utility given a desired goal. This is called rational behavior. This expects the agents to perform optimally, which may not be possible due to several constraints: partially observable environment, limited computational resources, rules implied by the task, etc. We can come up with a level of performance or bound that we want the agent to meet (for example, 60% win rate in chess); this is known as bounded optimallity. With bounded optimality we can expect the agent to not operate optimally, but still serves as a practical way of quantifing and measuring 

Game playing
------------

- Adversarial Search (Supplemental material: AI A modern approach, section 5.1, 5.2)

	- Competitive environments in which agents' goals are in conflict. Adversarial search, also known as games.
	- Optimal move and algorithms to finding it.
	- Pruning allows us to ignore portions of the search tree that make no difference to the final choice.
	- Heuristic evaluation functions allow us to approximate the true utility of a state without doing a complete search.
	- A game can be defined as a kind of search problem with the following elements:
		- So: 				Initial state, specifies how the game is set up at the start.
		- Players(s): 		Defines which player has the move in a state.
		- Actions(s):		Returns the set of legal moves in a state.
		- Results(s, a):	The transition model, which defines the result of a move (action).
		- Terminal-Test(s):	True when the game is over, false otherwise. Terminal states are states where the game ended.
		- Utility(s, p):	Utility function that defines the final numeric value for a game that ends in terminal state for player p.
	- A zero-sum game is defined as one where the total payoff to all players is the same for every instance of the game.
	- The initial state, actions function, and results functions define the game tree for the game -- a tree where the nodes are game states and the edges are moves.
	- Multiplayer games usually involve alliances, wheter formal or informal, among players.
	- If the game is not zero-sum, then collaboration can also occur with just two players. For example, consider that there is a terminal state with utilities. Then the optimal strategy is for both players to do everything possible to reach that state -- that is, the players will automatically cooperate to achieve a mutually desirable goal.
	
- Mini-max algorithm

	- Consider games with two players: Max and Min.
		- Max moves first, and then they take turns until the game is over.
	- Once we have a game tree, we label each terminal node with a value for a loss or for a win. We can then mark each tree node with a triangle: pointing up to indicate that the agent is trying to maximize its scores, making it a maximize node; pointing down when the opponent is trying to minimize the score, making it a minimize node.
	- For each max node, pick the maximum value among its child nodes.
	- For each min node, pick the minimum value amont its child nodes.
	- Given a game tree, the optimal srategy can be determined from the minimax value of each node, which is written as Minimax(n).
	- The minimax algorithm performs a complete depth-first exploration of the game tree. If the maximum depth of the tree is d and there are b level moves at each point, then the time complexity of the minimax algorithm is O(b^d). Although intractable due to time costs, this algorithm serves as a basis for the mathematical analysis of games and for more practical algoritms.

- Isolation game

	- Two players have a piece that they have to move in a squared board with several boxes. Both players can start where they want. After the initial placement, players can move the piece like a queen in chess: horizontally, vertically, and diagonally. When moving the piece, players can not move through the other player's piece, or through boxes that have already been occupied.
	- When a player moves, only the box where the player moved is marked as occupied, not the boxes traversed.
	- The objective is to be the last player to move, thus, the objective is to isolate the other player.
	- Branching factor: The fact that after a game progresses, there may be a more reduced number of possible moves than originally expected. Since in isolation from the third move you can move only as a queen in chess would move, the maximum number of places to go, assuming you are at the center, is 16. Generally, this is 12. 
	
	- Depth Limited Search: Limit the search space to a determined point to choose a move quickly. In this case, limit the depth of a search tree.
	
	- Quiescent Search: When the recommended branches are not changing much, so it is an indication that the choice may be a good one.
		- A problem that may arise is that you sometimes have to search several more levels if the results change too much between levels.
	
	- Iterative deepening: Doing a search and evaluating at each level (propagating the evaluation to all the levels), storing the result. If there is time remaining to make a choice, continue to the next level, and reevaluate the recommended solution. Quiescence is an ideal result, although not always the case.
		- Each iteration, we evaluate the tree up to a certain level, but with each level added, we need to revisit and reevaluate nodes. The nodes visited at each level is: 
		n = (b ^ (d+1) - 1) / (b - 1), where b is the branching factor, and d is the depth. The iterative deepening nodes are a sum of the visited tree nodes.
		- For some problems, iterative deepening is almost free because of the exponential level of the problem: you evaluate less nodes than the total amount of nodes you would have had to evaluate.
		- We can create strategies for how deep we want to search, depending on the game, and its state.
		- Horizon effect: when it is obvious to a human player that the game will be decided in the next move, but the computer can not search deep enough into the future to figure out the problem.
		
	- Evaluation functions play a key role in the game, and several can be tested to improve the performance.
	
	
- Alpha-beta pruning (Supplemental material: AI A modern approach, section 5.3)

	- Allows us to ignore whole sections of the game tree, and still get the same results as minimax.
	- A better explanation of the steps for the algorithm can be found in the book, Figure 5.5. 
	- Minimax with alpha-beta pruning, when successors are examined in random order rather than best-first, has a complexity of O(b^(3d/4)). For best-first, it has a complexity of O(b^(d/2))
	- The effectiveness of alpha-beta pruning is higly dependent on the order in which the states are examined.
	- Adding dynamic move-ordering schemes, such as trying first the moves that were found to be best in the past (could be the previous move, or could come from previous exploration of the current move), brings us quite close to the theoretical limit ( O(b^(d/2)) ).
	- Although the alpha-beta allows the pruning of large parts of a search tree, it still has to search all the way to terminal states for at least a portion of the search space. It was proposed insted to use heuristic evaluation functions to cut off the search earlier, effectively turning nonterminal nodes into terminal leaves.
		- This means that the utility function is replaced by a heuristic evaluation function EVAL, which estimates the position's utility, and replace the terminal test by a cutoff test that decided when to apply EVAL.
	- Strategies to reduce the game space: Partition, simetry, alpha-beta, hash table of good opening moves, reflecting moves.
	
	- Evaluation functions (Supplemental material: AI A modern approach, section 5.4)
		
		- An evaluation function returns an estimate of the expected utility of the game from a given position. The performance of a game-playing program depends on the quality of its evaluation function, which makes its crucial to define good evaluation functions:
			- First, evaluation functions should order the terminal states in the same way as the true utility function: states that are wins must evaluate better than draws, which in turn must evaluate better than losses. 
			- Second, the evaluation function must not take too long. 
			- Third, for the nonterminal states, the evaluation function should be strongly correlated wih the actual chances of winning. Chances of winning: if the search must be cut off at nonterminal states, the algorithm will necessarily be uncertain about the final outcomes of those states.
		- Most evaluation functions work by calculating various features of the state: pieces on the board, types of pieces, moves, spaces, etc. 
		- The features, taken together, define categories or equivalence classes of states: the states in each category have the same values for all the features.
			- In practice, this kind of analysis requires too many categories and too much experience to estimate the probabilities of winning.
		- Most evaluation functions compute separate numerical contributions from each feature and then combine them to find the total value. This kind of evaluation function is called weighted linear function and can be expressed as:
			EVAL(s) =  sum_{i=1}^{n} wi fi(s)
		where wi is a weight and fi is a feature of the position. this assumes that each feature is independent of the values of other features. For example, assigning the value 3 to a bishop in chess ignores the fact that bishops are more powerful in the endgame, when they have more space to maneuver. This is why some games use nonlinear combinations of features.
		
	- Cutting off search (Supplemental material: AI A modern approach, section 5.4.2)
	
		- The next step is to modify ALPHA-BETA-SEARCH so that it will call the heuristic EVAL function when it is appropriate to cut off the search. 
		- The evaluation function should be applied only to positions that are quiescenet -- that is, unlikely to exhibit wild swings in value in the near future.
		- Horizon effect: It arises when the program is facing an opponent's move that causes serious damage and is ultimately unavoidable, but can be temporarily avoided by delaying tactics. 
			- One strategy to mitigate this effect is the singular extension, a move that is "clearly better" than all other moves in a given position. Once discovered anywhere in the tree in the course of a search, this singular move is remembered. When the search reaches the normal depth limit, the algorithm checks to see if the singular extension is a legal move; if it is, the algorithm allows the move to be considered.
	
	- Forward prunning
	
		- Some moves at a given node are pruned immediately without further consideration. 
		- Beam serach: on each level, consider a "beam" of the n best moves (according to the evaluation function) rather than considering all possible moves. This approach does not guarantee that the best move will not be pruned.
		- The probabilistic cut (probcut) is a forward-prunning version of alpha-beta search that uses statistics gained from prior experience to lessen the chance that the best move will be pruned.
		
- Isolation game evaluation heuristics

	In order to evaluate the utility of a non-leaf node in the tree, we used a combination of the number of free spaces on the board and the number of legal moves available to each player. The number of free spaces on that board gives an indication of how much of the board is filled up. It is not always necessary that the legal moves would cover every square on the board and this metric is used to find how “isolated” a player is and whether there is opportunity to move to a more open area. The number of legal moves available to each player are used to not only maximize the possibilities of oneself, but also minimize the possibilities for others. The agents used (number of current player’s moves - number of enemy’s moves)/(num empty spaces +1). 

Problem Solving
---------------
Search
------

The theory and technology of building agents that can plan ahead to solve problems.

- Definition of a problem

	- Initial state of the problem s_0
	- Possible actions that an agent can take on a given state {a_1, a_2, ... }
	- Result function (s, a) which returns a new state s' = (s, a)
	- Function goalTest (s) returns a bool, tests if a given state is the goal of the problem
	- Path Cost function which takes a path (sequence of state action transitions) and returns a number, which is the cost of the path. Most functions are additive, which means the cost of a path is the sum of the cost of individual steps.
	- Step cost function, takes a state, and action, and a resulting state, and returns a number n, which is the cost of that step.

	- The state space are all the possible states, and it can be navigated by applying actions.
	- At every point we want to separate the state out into three parts: The frontier is the end state of the paths that have been explored, the previous states are the explored states of the state space, and the other states are the unexplored part of the state space.

- Breadth-First Search 
	
	- Shortest First Search: always choose from the frontier one of the paths that has not been considered yet, which is the shortest. So fat, shortest has meant the one that has the fewer steps, not necessarily the least cost.
	
- Uniform cost Search

	- Breath-first search but expands considering the lowest cost path, instead of the path with the least states.

- Greedy best-first search
	- A* algorithm: Expanding the path that has the minimum value for the funciton f = g + h; where g(path) = path cost, h(path) = h(s) = estimated distance to the goal (heuristic estimate function)
	- A* will find the lowest cost path if the h function for a state is less than the true cost of the path to the goal through that state. H should never overestimate the distance to the goal. H is optimistic; h is admissible to find the lowest cost path.
	- The "intelligent" part of a problem is the definition of the heuristic function, which is normally provided. The idea is to automatically come up with good heuristics given the definition of a problem.
	- By generating a relaxed problem (removing constraints of a problem) we can derive admissible candidate heuristics. To come up with a good heuristic, we can say that h = max(h1, h2, ..., hn), which never overestimates and is guaranteed to be better because it is getting closer to the target. The problem is that there is a cost to compute the heuristic, which can be longer to compute even if in the end fewer paths are expanded.		
	
- Completeness: If there is a goal, an algorithm finds a path to that goal.
	- Breath first and cheapest first are complete. Depth first is not; if there is an infinite path it may not get to the path with the goal.

- Search works when
	- The space is fully observable: Must be able to see what initial state we start out with.
	- Known: Must know the set of available actions.
	- Discrete: Must be a finite number of actions to choose from.
	- Deterministic: Must know the results of taking an action.
	- Static: Must be nothing else than can change the world other than our own actions.
	
- Implementation details
	- Data structure for the paths: Node
		- Current state (state at the end of the path)
		- Action it took to get there
		- Total cost
		- Pointer to the parent
	- A path can be a linked list of Nodes
	- The frontier
		- Operations
			- Remove best items
			- Add in new ones
			- Membership test
		- Implementation
			- Priority queue
		- Representation
			- Set
		- Build
			- Hash table
			- Tree
	- Explored
		- Operations
			- Add new members
			- Check for membership
		- Representation
			- Single set
		- Build
			- Hash table
			- Tree


- Simulated Annealing

	- For problems where it is not that easy to find a solution, such as where the state space is very large.
	- There are a whole class of problems where just a little bit of intelligence and iteratively improving the solution gets you very close to an optimal solution. Iteratevely improving algorithms (random restart, genetic algorithms, simulated annealing), An element of randomness is what you need! All these algorithms take a randomized approach, and thus avoid getting stuck in local minima.
	- Random restart: restart at different points, try to sole the problem, and return the best solution of each iteration.
		- As the number of iterations goes to infinity, it should find the best solution. A taboo search avoids places you have already been to by storing the places you have been to and restarting if you get to the same place. Another approach would be to store all the local maximums, and try to estimate where the next one might be.
		- Special attention has to be taken for the step size: too small and it might return before a considerable change is found, too large, and it might miss possible solutions.
	- Local beam search, instead of using only one particle, we use k-particles, and on each iteration, the information of the neighbors is also considered to point to the solution. Stochastical beam search.
	- Genetic algorithms. These use mutation and breeding to find a solution.
	
- Constraint satisfaction problems

	- Variables, domains, constraints
	
- Logic and planning

	- Propositional Logic
		- True, false, or unknown facts
		- v: or
		- >: implies (if p then q)
		- ^: and
		- <>: bidirectional implies
		- ¬: not
		- Valid: Always true, Satisfiable: sometimes tru, unsatisfiable: never true

	- First order logic
		- True, false, or unknown relationships about things in the world
		- Models that define the world are more complex, consisting on a set of objects, constants (made up of references to objects), functions which are mappings of objects to other objects, and sets of relations between objects. Every model has to have at least one object.
		- Satisfiability is that it can be true in some models, not true in all models.
		- Syntax
			- Sentences: describe facts that are true of false. The operators of propositional logic can be applied to sentences. Additionally, there is an equality relationship.
			- Terms: describe objects. Can be constants, variables, and functions.
			- Quantifiers: For all, and there exists. If the quantifier is ommited, it is assumed that it is the for all quantifier.
		
	- Probability theory
		- Facts in the range of [0...1]
		
- Planning Search

	- Problem solving does all the planning work first, before executing the plan.
	- For planning we need feedback from the environment, we need to interleave planning with execution.
	- Because of properties of the environment that make it difficult to deal with. 
		- Most important one is if the environment is stochastic: We do not know for sure what an action is going to do; we have do deal with contingencies. With stichastic worlds we can have a next state space with more states than before.
			- In stochastic partially observable environments, the actions tend to increase uncertainty and the observations tend to bring that uncertainty back down.
		- Multiagent systems.
		- Partial observability: we dont know what stage we are in, until we are there to ask the state.
		- Basically, lack of knowledge.
		- Hierarchical plans.
		- To counter these, we have to plan with belief states. 
	- Belief states. 
		- We start in a space that can contain all other possible states, and by executing actions, we gain knowledge of the world and the possible states we are in.
		- It is possible to form a plan without ever having had to sense the world: Conformant plans.
		- Local sensing: Can see what location it is in, can see what is going on in the current location.
		- In the end, it was all state machines...
		- To find a successful plan, we can still use search, only the tree is more complicated.

	- Classical planning
		- Planning Domain definition language: Describes the four things we need to define a search problem
			- Initial state
			- The actions that are available in a state
			- The result of applying an action
			- The goal test
		- Each state is represented as a conjunction of fluents that are gound (can not be arbitrary variables; variable-free), functionless atoms.
		- Actions are represented by action schemas.
		- state space: all possible assignments to k-boolean variables, so there will be 2^k states in the state space
		- World state: complete assignment of true or false to each variable.
		- Belief state: complete assignments, partial assignments, arbitrary formula
		- action schema: represents many similar actions that are similar to each other. 
			- Action(Fly(p, from, to)
				Preconditions: Plane(p) ^ Airport(from) ^ Airport(to) ^ At(p,from)
				Effect: ¬At(p, from) ^ At(p, to))
			- It represents a set of actions for all planes, all from's and to's, it also says what we need to know to take that action and the effect of taking that action.
			- Actions schemas help to create good heurists by relaxing (removing) certain preconditions or effects.
			
		- Planning algorithms
			- Forward search: Regular search algorithm that starts at the initial state. However, forward state-space search is too inefficient to be practical for planning: It is prone to exploring irrelevant actions, and planning problems often have large state spaces.
			- Backward (regression) relevant-states search: In regression search we start at the goal and apply the actions backward until we find a sequence of steps that reaches the initial state. It is called relevant-states search because we only consider actions that are relevant to the goal (or current state). As in belief-state search, there is a set of relevant states to consider at each step, not just a single state.
			- Backward search works only when we know how to regress from a state description to the predecessor state description. The PDDL representation was designed to make it easy to regress actions.
			- Given a ground goal description g and a ground action a, the regression from g over a gives us a state description g' defined by:
			g' = (g - ADD(a)) U Precond(a)
			- That is, the effect that were added by the action need not have been true before, and also the preconditions must have held before, or else the action could not have been executed.
			- In forward direction we choose action that were applicable-those actions that could be the next step in the plan. In backward search we want actions that are relevant-those actions that could be the last step in a plan leading up to the current goal state.
		
		
		- Heuristisc for planning
			- An admisible heuristic can be derived by defining a relaxed problem that is easier to solve. The exact cost f a solution to this easier problem then becomes the heuristic for the original problem.
			- An example would be the ignore predonditions heuristic drops all preconditions from actions.
				- First, we relax the actions by removing all preconditions and all effects except those that are literals in the goal. Then, we count the minimum number of actions required such that the union of those actions' effects satisfies the goal. This is an instance of the NP-hard set-cover problem.
			- It is also possible to ignore only selected preconditions of actions.
			
----------------------------

Probability
----------

Challenge question:

3% of the population has X disease. A drug Y can detect it but not with 100% accuracy. If a person has X, there is a 1% chance that it will go undetected. There is also a 10% chance of false alarms. If A takes Y which returns positive, what is the probability of adam having the disease?

P(X) = 0.03 -> 3% of the population has X disease
p(~Y/X) = 0.01 -> if a person has X, there is a 1% chance it will go undetected
p(Y/~X) = 0.10 -> 10% chance of false positive

p(X/Y) = P(Y/X) P(X) / P(Y) -> Bayes rule

P(Y/X) = 1 - P(~Y/X) = 0.99
P(Y) = P(Y/X)P(X) + P(Y/~X)P(~X) -> Probability of Y is equals to the probability of Y happening within a population plus the probability of Y happening outside the population. That is why P(Y/X) is multiplied by P(X); P(Y/X) is the chance of Y happening, and P(X) is the chance of it happening in the population. The same case applies for the other factor in the equation.
P(Y) = 0.99 * 0.03 + 0.1 * 0.97 = 0.1267

P(X/Y) = 0.99 * 0.03 / 0.1267 = 0.234

- Intro to probability and bayes networks

	- Bayes Networks: A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG).
	
	- Complementary probability: P(A) = p -> P(¬A) = 1 - p
	
	- Independence (unicode character ⊥ = u+22a5): X⊥Y : P(X)P(Y) --marginals-- = P(x, Y) --joint probability--
	
	- Joint probability of independent variables: P(X,Y) = P(X) P (Y)
	
	- Joint probability of dependent variables: P(X,Y) = P(Y|X) P(X) = P(X|Y) P(Y) ( https://www.khanacademy.org/math/statistics-probability/probability-library/conditional-probability-independence/v/calculating-conditional-probability ; https://en.wikipedia.org/wiki/Joint_probability_distribution )
	
	- Dependence: P(X | Y) -> Probability of X given that Y happened: 
	When two events, A and B, are dependent, the probability of both occurring is: P(A, B) = P(A) P(B|A)
	
	- Conditional probability: P(A|B) = P(A,B) / P(B) 
	
	- Total probability (unicode character ∑ a+2211): P(Y) = ∑_i P(Y|X = i) P(X=i), or, equivalently, ∑_i P(A) = P(A, B_i)
	
	- P(¬X|Y) = 1 - P(X|Y) NOTE: P(X|¬Y) != 1 - P(X|Y)
		
	- Conditional Independence: Knowing anything about a world T1 would not help make a statement about another world T2, given that we know the value of C. P(T2|C,T1) = P(T2|C).
	In probability theory, two events R and B are conditionally independent given a third event Y precisely if the occurrence of R and 		the occurrence of B are independent events in their conditional probability distribution given Y. In other words, R and B are 		conditionally independent given Y if and only if, given knowledge that Y occurs, knowledge of whether R occurs provides no 		information on the likelihood of B occurring, and knowledge of whether B occurs provides no information on the likelihood of R 		occurring.
	P(R, B|Y) = P(R|Y)P(B|Y) or, equivalently, P(R|B,Y) = P(R|Y)
	
	- Bayes rule
	
		- P(A|B) = P(B|A) P(A) / P(B)
		- P(B) = P(B|A) P(A) + P(B|¬A) P(¬A)
			- P(A|B) + P(¬A|B) = 1
			- P(A|B) = P(B|A) P(A) / P(B)
			- P(¬A|B) = P(B|¬A) P(¬A) / P(B)
			- P(B|A) P(A) / P(B) + P(B|¬A) P(¬A) / P(B) = 1
			- P(B) = P(B|A) P(A) + P(B|¬A) P(¬A)
	
			
		
	
	
	
	
	
	
	



